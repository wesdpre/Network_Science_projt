{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Network Science Homework 1**\n",
    "## **Key Network Properties, Graph Models and Gephi**\n",
    "\n",
    "### **Student:**\n",
    "- Pedro Azevedo, up201905966@up.pt\n",
    "- Davi Santos, up202310061@up.pt\n",
    "- , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 1)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an undirected graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Create graph with named nodes\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges using named nodes\n",
    "G.add_edges_from([\n",
    "    (\"A\", \"B\"),\n",
    "    (\"A\", \"C\"),\n",
    "    (\"B\", \"E\"),\n",
    "    (\"B\", \"C\"),\n",
    "    (\"C\", \"E\"),\n",
    "    (\"C\", \"D\"),\n",
    "    (\"D\",\"F\"),\n",
    "])\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(6, 4))  # Optional: control the figure size\n",
    "nx.draw(\n",
    "    G,\n",
    "    with_labels=True,         # Show node labels\n",
    "    node_color='lightgreen',  # Node color\n",
    "    node_size=800,            # Node size\n",
    "    font_size=14,             # Label font size\n",
    "    font_weight='bold',\n",
    "    edge_color='gray',\n",
    "    width=2\n",
    ")\n",
    "plt.title(\"Graph with Named Nodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Show the degree of each node and make a plot of its (normalized) degree distribution.\n",
    "\n",
    "| Degree (k) | N(k) |\n",
    "|------------|------|\n",
    "| 1          | 1    |\n",
    "| 2          | 3    |\n",
    "| 3          | 1    |\n",
    "| 4          | 1    |\n",
    "\n",
    "normalized:\n",
    "P(k) = N(k) / N, where N = 6\n",
    "\n",
    "| Degree (k) | P(k)        |\n",
    "|------------|-------------|\n",
    "| 1          | 1 / 6 = 0.16 |\n",
    "| 2          | 3 / 6 = 0.5 |\n",
    "| 3          | 1 / 6 = 0.16 |\n",
    "| 4          | 1 / 6 = 0.16 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Compute degrees\n",
    "degrees = [deg for node, deg in G.degree()]\n",
    "degree_counts = Counter(degrees)\n",
    "total_nodes = G.number_of_nodes()\n",
    "normalized_distribution = {k: v / total_nodes for k, v in degree_counts.items()}\n",
    "\n",
    "# Plot normalized degree distribution\n",
    "degrees_sorted = sorted(normalized_distribution)\n",
    "values_sorted = [normalized_distribution[d] for d in degrees_sorted]\n",
    "\n",
    "print(f\"degrees frequency: {degree_counts}\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(degrees_sorted, values_sorted, color='salmon')\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Normalized Frequency\")\n",
    "plt.title(\"Normalized Degree Distribution\")\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (b) Calculate the diameter and the average path length of the network\n",
    " \n",
    " A: \n",
    " - diameter = distance BF(hB-F) = 3\n",
    " - avg path lenght\n",
    " \n",
    "| Nodes | Distance |\n",
    "|-------|----------|\n",
    "| AB    | 1        |\n",
    "| AC    | 1        |\n",
    "| AE    | 2        |\n",
    "| AD    | 2        |\n",
    "| AF    | 3        |\n",
    "| BC    | 1        |\n",
    "| BE    | 1        |\n",
    "| BD    | 2        |\n",
    "| BF    | 3        |\n",
    "| CD    | 1        |\n",
    "| CE    | 1        |\n",
    "| CF    | 2        |\n",
    "| ED    | 2        |\n",
    "| EF    | 3        |\n",
    "| DF    | 1        |\n",
    "|       |          |\n",
    "| **Total** | (1+1+2+2+3+1+1+2+3+1+1+2+2+3+1) = 26 |\n",
    "| **Total pair nodes** | 15 |\n",
    "| **AvgPthLen** | 26 / 15 = **1.7333** |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter = nx.diameter(G)\n",
    "avg_path_length = nx.average_shortest_path_length(G)\n",
    "print(\"Diameter of the network:\", diameter)\n",
    "print(\"Average shortest path length:\", avg_path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Calculate the local clustering coeffcient of each node and the average local clustering coeffcient of the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Node        | k | e | C = 2e / k(k−1)                        |\n",
    "|-------------|---|---|----------------------------------------|\n",
    "| A           | 2 | 1 | 2×1 / (2×(2−1)) = 1                    |\n",
    "| B           | 3 | 2 | (2×2) / (3×(3−1)) = 0.6667             |\n",
    "| C           | 4 | 2 | (2×2) / (4×(4−1)) = 0.3333             |\n",
    "| D           | 2 | 0 | (2×0) / (2×(2−1)) = 0                  |\n",
    "| **Cluster Avg** |   |   | (1 + 0.6667 + 0.3333 + 0) / 4 = 0.5 |\n",
    "<!--  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local clustering coefficient for each node\n",
    "clustering_dict = nx.clustering(G)\n",
    "print(\"Local Clustering Coefficients:\")\n",
    "for node, coef in clustering_dict.items():\n",
    "    print(f\"{node}: {coef:.4f}\")\n",
    "\n",
    "# Average clustering coefficient\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "print(f\"\\nAverage Clustering Coefficient of the network: {avg_clustering:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (d) Calculate the (normalized) betweenness centrality and closeness centrality of each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Node | Betweenness Value (Non-Normalized)               | \n",
    "|------|--------------------------------------------------|\n",
    "| A    | 0                                                |\n",
    "| B    | 0.5 (AE)                                         |\n",
    "| C    | 0.5 (AE) + 3 (A\\|B\\|E–D) + 3 (A\\|B\\|E–F) = 6.5    |\n",
    "| D    | 4 (A\\|B\\|C\\|E–F)                                 |\n",
    "| E    | 0                                                |\n",
    "| F    | 0                                                |\n",
    "\n",
    "\n",
    "| Node | Betweenness Value (Normalized)                        | Closeness (Normalized)                    |\n",
    "|------|-------------------------------------------------------|-------------------------------------------|\n",
    "| A    | 0                                                     | 1 / ((1+1+2+2+3)/5) = 0.5556              |\n",
    "| B    | 0.5 / ((6−1)(6−2)/2) = 0.05                           | 1 / ((1+1+1+2+3)/5) = 0.625               |\n",
    "| C    | 6.5 / ((6−1)(6−2)/2) = 0.65                           | 1 / ((1+1+1+1+2)/5) = 0.8333              |\n",
    "| D    | 4 / ((6−1)(6−2)/2) = 0.4                              | 1 / ((1+1+2+2+2)/5) = 0.625               |\n",
    "| E    | 0                                                     | 1 / ((1+1+2+2+3)/5) = 0.5556              |\n",
    "| F    | 0                                                     | 1 / ((1+2+3+3+3)/5) = 0.4167              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non normalized betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G, normalized=False)\n",
    "print(\"Non-Normalized Betweenness Centrality:\")\n",
    "for node, value in betweenness.items():\n",
    "    print(f\"{node}: {value:.4f}\")\n",
    "\n",
    "# Compute betweenness centrality (normalized)\n",
    "betweenness = nx.betweenness_centrality(G, normalized=True)\n",
    "print(\"Normalized Betweenness Centrality:\")\n",
    "for node, value in betweenness.items():\n",
    "    print(f\"{node}: {value:.4f}\")\n",
    "\n",
    "# Compute closeness centrality (normalized)\n",
    "closeness = nx.closeness_centrality(G)\n",
    "print(\"\\nNormalized Closeness Centrality:\")\n",
    "for node, value in closeness.items():\n",
    "    print(f\"{node}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 2)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transitivity gives more importance to the high degree nodes. This because the high degree nodes have a higher impact in the global occurrence of triplets, meaning higher impact in the transitivity. It is not true for average clustering coefficient, since it is calculated using the mean, giving equal weight for all the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using Gephi**\n",
    "For this exercise you will be asked to analyze a flight network obtained from the [openflights dataset](https://openflights.org/data.php). Start by downloading a zip file containing the network in gexf format.\n",
    "http://www.dcc.fc.up.pt/˜pribeiro/aulas/ns2425/homework/flights.zip\n",
    "Open the network in Gephi and use Geo Layout to visualize (you may need to install that plugin). The network is directed and corresponds to a multigraph (multiple edges between the same airports, corresponding to different airlines). When loading the network you will be asked about edge merge strategy: not merging will keep the multigraph, merging with ”sum” will create a weight attribute corresponding to the number of flights). Different questions might require different merging strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ex 3 )**\n",
    "\n",
    "Answer to the following questions. For each one give a brief explanation of the steps you took. For top-k like questions you need to show the name and city of the airports and also the respective numbers (ex: number of flights or centrality value). If there are ties, show all tied answers on the k first positions. \n",
    "\n",
    "You might need to use various features of Gephi (appearance, layouts, filters and statistics). If needed,\n",
    "you can just reload the network to have a fresh restart.\n",
    "\n",
    "\n",
    "### **(a)**  **Question**: What is the number of airports and flights in the network?\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The number of airports corresponds to the number of nodes = 3147 and the number of flights corresponds to the number of edges (not merged) = 66679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(b)**  **Question**: On average, an airport has how many outgoing flights? And to how many different airports?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "On a merged edges dataset(sum) the mean number of outgoing flights corresponds to the Mean Weighted Outdegree = 21,188. On, average the flights goes to 11,699 airports (Mean Outdegree). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c)**  **Question**: What is the diameter and average path length of the network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The diameter is 13 – It means that the longest shortest route between 2 airports is 13 flights – and the average path length is 3,969 – it means that we need on average ~4 flights to travel between 2 airports in the network. \n",
    "\n",
    "![imagem 1](docs/img/imag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(d)**  **Question**: List the top-5 pairs of airports with more flights between each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "On a merged edges dataset(sum) we can list the top 5 pairs of airports with+ more flights by sorting for Weight. We imported the dataset to python and then merged the nodes dataset (containing the name of the airports) and the edges dataset (containing the ID of the source and target airports). Then we sorted by the column “Weight” and because there were ties we have obtained 28 values: \n",
    "\n",
    "![image 2](./docs/img/imag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(e)**  **Question**: List the top-5 of the airports that have flights to the highest number of other airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The top 5 flights to the highest number of other airports should be checked on the value of Unweighted Out-degree: \n",
    "\n",
    "\n",
    "![image 2](./docs/img/imag3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(f)**  **Question**: List the top-5 of the airports with highest normalized betweenness centrality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The top 5 flights with highest normalized betweenness centrality are: \n",
    "\n",
    "These are big airports that often act like “bridges” on other flights. \n",
    "\n",
    "![image 2](./docs/img/imag4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(g)**  **Question**: Consider Ted Stevens Anchorage International Airport. What is its global ranking in terms of betweenness centrality and out-degree? Can you explain the discrepancy? Indicate two other airports with the same kind of behavior (high betweenness centrality but relatively low out-degree).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "Ted Stevens Anchorage International Airport is ranked 5th in the top airports by betweenness centrality, but 280th in the out-degree ranking. This indicates that the airport has relatively few departing flights yet plays an important role in the global network. It is likely situated along major flight paths that connect multiple regions, serving as a key transit or connecting hub. In this same situation is “Guarulhos - Governador André Franco Montoro International Airport” and “Sydney Kingsford Smith International Airport”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(h)**  **Question**: List the top-5 of countries with the highest number of airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The top 5 countries with more airports are United States, Canada, China, Brazil and Australia. \n",
    "\n",
    "![image 5](./docs/img/imag5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(i)**  **Question**: List the top-5 of airlines with the highest number of flights. [(info about airline codes)](https://openflights.org/data.php#airline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "The top 5 airlines with the highest number of flights are: \n",
    "\n",
    "![image 6](./docs/img/imag6.png)\n",
    "\n",
    "To observe these values, we didn’t merge the edges when importing the file to Gephi.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and output copyed from the src/rede.ipymb \n",
    "\n",
    "# Get list of IATA codes for Chinese airports\n",
    "china_airports = set(df_nodes_china['iata'])\n",
    "\n",
    "# Filter edges where both source and destination are in China\n",
    "china_domestic_flights = df_edges[\n",
    "    (df_edges['s'].isin(china_airports)) &\n",
    "    (df_edges['d'].isin(china_airports))\n",
    "]\n",
    "\n",
    "len(china_domestic_flights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(j)**  **Question**: What is the number of domestic flights inside China?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    " For getting the number of domestic flights inside China we analyze the data without merging edges. Then, with a dataset with the nodes (airports) of China, we were able to merge to the edge’s dataset only if the source column “s” and the destination column “d” correspond to the IATA code of a airport in China. We find 6851 domestic flights. Here’s the code we use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and output copyed from the src/rede.ipymb \n",
    "\n",
    "len(df_nodes_merged_USA.loc[df_nodes_merged_USA[\"outdegree\"] >= 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(K)**  **Question**: What is the number of domestic flights inside China?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "To check this, we used the merged edges dataset with only the USA country and we filtered the outdegree column (=> 50). We found 32 airports in USA that fly to at least 50 airports in the network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code and output copyed from the src/rede.ipymb \n",
    "\n",
    "df_nodes_portugal = df_nodes.loc[df_nodes[\"country\"] == \"Portugal\"]\n",
    "df_nodes_brazil = df_nodes.loc[df_nodes[\"country\"] == \"Brazil\"]\n",
    "\n",
    "portugal_airports = set(df_nodes_portugal['iata'])\n",
    "brazil_airports = set(df_nodes_brazil['iata'])\n",
    "\n",
    "PTtoBR_flights = df_edges[\n",
    "    (df_edges['s'].isin(portugal_airports)) &\n",
    "    (df_edges['d'].isin(brazil_airports))\n",
    "]\n",
    "\n",
    "BRtoPT_flights = df_edges[\n",
    "    (df_edges['s'].isin(brazil_airports)) &\n",
    "    (df_edges['d'].isin(portugal_airports))\n",
    "]\n",
    "\n",
    "print(len(PTtoBR_flights)+len(BRtoPT_flights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(L)**  **Question**:  How many flights are there between Portugal and Brazil?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "For getting the number of flights between Portugal and Brazil we analyzed the nodes and edges dataset not merged. The first step was to filter flights that departure from Portugal to Brazil and then the opposite. We did this in a similar way that we did with China in the question J). We found 24 values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(M)**  **Question**: Consider a network formed only by Ryanair flights. What is the number of nodes and edges of its giant component? Considering only this giant component, what is the most important airport in terms of closeness centrality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(n)**  **Question**: How many airport are reachable from Francisco de S´a Carneiro Porto Airport in 1 flight? And in at most 2 flights? And in at most 3 flights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(o)**  **Question**: Create an image showing the flight network between american and canadian aiports with more than 100 destinations in the global network. The size of the nodes should reflect the global betweenness centrality, and their colors should be different for each time zone. Nodes should be labeled with the city name. Try to make your image as comprehensible and aesthetically pleasing as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Erdos-Renyi Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ex 4)**\n",
    "\n",
    "**Question**:\n",
    "\n",
    "Write code for generating a random network following the G(n,p) Erdos-Renyi model, that is,\n",
    "a network with n nodes, where each pair of nodes has probability p of being connected.\n",
    "Include as attached files, two random networks random1.txt and random2.txt generated respectively\n",
    "with n = 2000, p = 0.0001 and n = 2000, p = 0.005 (in the described format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "We can solve this exercise in two ways:\n",
    "- 1st by creating our own code, in this case it was created in the file \"random_network.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.random_erdos_renyi_network import generate_erdos_renyi\n",
    "\n",
    "# Generate a Network Erdos Renyi and save\n",
    "graph1 = generate_erdos_renyi(n=2000, p=0.0001, filename=\"data/random1.txt\")\n",
    "graph2 = generate_erdos_renyi(n=2000, p=0.005, filename=\"data/random2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2nd we can also create a random network following Erdos-Renyi Model using the [networkx](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.erdos_renyi_graph.html#rf62e773f8347-2) library which contains a specific function for this purpose \n",
    "\n",
    "ps. Networkx recommends using another function that is not as complex. [fast_gnp_random_graph](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.fast_gnp_random_graph.html#networkx.generators.random_graphs.fast_gnp_random_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from src.load_save_network import save_network_nx\n",
    "\n",
    "graph1 = nx.erdos_renyi_graph(n=2000, p=0.0001)\n",
    "graph2 = nx.erdos_renyi_graph(n=2000, p=0.005)\n",
    "\n",
    "# Save the graphs to files\n",
    "save_network_nx(graph1, \"data/random1.txt\")\n",
    "save_network_nx(graph2, \"data/random2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 5)** \n",
    "\n",
    "**Question**:\n",
    "\n",
    "Write code for computing the size of the giant component of a given network.\n",
    "Use it to compute the size of giant component of the two random networks you generated (random1.txt\n",
    "and random2.txt), and show the obtained results.\n",
    "Hint: you can use any graph method traversal to compute the giant component, such as breadth-first\n",
    "search (BFS) or depth-first search (DFS). Take care to implement an O(n) algorithm that only passes\n",
    "trough each node once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "We try to solve this exercise in 3 different ways, there was a problem all give different values:\n",
    "\n",
    "after loading graph\n",
    "\n",
    "- 1st attempt creating our own code following the description on slide 10 of [Measuring Networks and\n",
    "Random Graph Models](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2425/2_graphmodels.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_save_network import load_network_advanced\n",
    "\n",
    "graph1 = load_network_advanced(\"data/random1.txt\", undirected_graph=False)\n",
    "graph2 = load_network_advanced(\"data/random2.txt\", undirected_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.giant_component import giant_component_size\n",
    "\n",
    "giant3 = giant_component_size(graph1)\n",
    "giant4 = giant_component_size(graph2)\n",
    "\n",
    "print(\"giant component size of random1 using model described in the slides =\", giant3)\n",
    "print(\"giant component size of random2 using model described in the slides =\", giant4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2nd attempt creating our own code for a more efficient code by eliminating already visited clusters so that we don't need to do a search if a node has been visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.giant_component import giant_component_size_TryRemove\n",
    "\n",
    "# Find the giant component of the first graph\n",
    "giant1 = giant_component_size_TryRemove(graph1)\n",
    "# Find the giant component of the second graph\n",
    "giant2 = giant_component_size_TryRemove(graph2)\n",
    "print(\"giant component size of random1 using TryRemove =\", giant1)\n",
    "print(\"giant component size of random2 using TryRemove =\", giant2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 3rd attempt was made after analyzing the results of the previous attempts in the hope of confirming the results. We used the networkx library to find the size of the giant component, but the result goes against previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def load_network(filename):\n",
    "    \"\"\"Loads a network from a file into a NetworkX graph.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"File {filename} not found.\")\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            u, v = map(int, line.strip().split())\n",
    "            graph.add_edge(u, v)\n",
    "    return graph\n",
    "\n",
    "G1 = load_network(\"random1.txt\")\n",
    "G2 = load_network(\"random2.txt\")\n",
    "\n",
    "# Find the largest connected component\n",
    "giant_component_1 = max(nx.connected_components(G1), key=len)\n",
    "giant_component_2 = max(nx.connected_components(G2), key=len)\n",
    "\n",
    "# Create subgraphs from the largest components\n",
    "giant_nx_1 = G1.subgraph(giant_component_1)\n",
    "giant_nx_2 = G2.subgraph(giant_component_2)\n",
    "\n",
    "print(\"Giant component size of random1 using NetworkX =\", len(giant_nx_1))\n",
    "print(\"Giant component size of random2 using NetworkX =\", len(giant_nx_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 6)**\n",
    "\n",
    "**Question**:\n",
    "\n",
    "The following is an exercise to study the emergence of a giant component.\n",
    "Combining your previous code, generate a series of random networks with n = 2000 and p varying\n",
    "from 0.0001 to 0.005 (with steps of 0.0001).\n",
    "\n",
    "Show a plot of your results, with the X axis representing p and the Y axis representing the size of the\n",
    "giant component.\n",
    "\n",
    "Was the plot what you were expecting? What is the shape of it? At what average degree values do\n",
    "you notice something happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.random_erdos_renyi_network import generate_erdos_renyi\n",
    "from src.load_save_network import load_network_advanced\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_giant_component(p_start, p_stop, p_step, n, filename, giant_component_func):\n",
    "    p_array_values = np.arange(p_start, p_stop, p_step)\n",
    "    #print(f\"p_array_values = {p_array_values}\")\n",
    "    \n",
    "    giant_sizes_array = []\n",
    "    for p in p_array_values:\n",
    "        #print(f\"p = {p:4f}\")\n",
    "        graph = generate_erdos_renyi(n=n, p=p, filename=filename, file_remove=True)\n",
    "        graph = load_network_advanced(filename)\n",
    "        giant_size = giant_component_func(graph)\n",
    "        giant_sizes_array.append(giant_size)\n",
    "        #print(f\"Giant component size for p={p:4f}: {giant_size}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(p_array_values, giant_sizes_array, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel(\"p\")\n",
    "    plt.ylabel(\" the size of the giant component\")\n",
    "    plt.title(\"A plot the results of Giant Component in an Erdős-Rényi Network\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.giant_component import giant_component_size\n",
    "\n",
    "p_start = 0.0001\n",
    "p_stop = 0.005\n",
    "p_step = 0.0001\n",
    "n = 2000\n",
    "\n",
    "plot_giant_component(p_start, p_stop, p_step, n, \"random.txt\", giant_component_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Barabasi-Albert Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 7)**\n",
    "\n",
    "**Question**:\n",
    "\n",
    "Write code for generating a random network following the $BAn,m0,m$ [Barab´asi-Albert model](https://en.wikipedia.org/wiki/Barab%C3%A1si%E2%80%93Albert_model).\n",
    "\n",
    "This model uses a preferential attachment mechanism and it works in the following way ([some hints](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2425/homework/bamodel.pdf)):\n",
    "\n",
    "- You begin with a fully connected network containing m0 nodes\n",
    "- In each iteration (until you reach a total $n$ nodes) you add one new node connected to m existing nodes, with a probability proportional to the number of already existing connections of previous nodes. Formally, the probability $p_i$ that the new node is connected to node $i$ is:\n",
    "\n",
    "$$\n",
    "p_i =\\frac{k_i} {\\sum_{j} k_i }\n",
    "$$\n",
    "\n",
    "$k_i$ is the degree of node i and the sum is made over the degrees of all existing nodes.\n",
    "\n",
    "Include as attached files, two barab´asi-albert networks ba1.txt and ba2.txt generated respectively\n",
    "with $n = 2000$, $m0 = 3$, $m = 1$ and $n = 2000$, $m0 = 5$, $m = 2$ (in the described format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "\n",
    "it was created a file barabasi_albert where we can generate a Barabasi-Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_save_network import save_graph_network\n",
    "from src.barabasi_albert import generate_barabasi_albert\n",
    "\n",
    "save_graph_network(generate_barabasi_albert(n = 2000, m0=3, m=1, random_seed=42), \"data/ba1.txt\")\n",
    "save_graph_network(generate_barabasi_albert(n = 2000, m0=5, m=2, random_seed=42), \"data/ba2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ex 8)**\n",
    "\n",
    "**Question**:\n",
    "\n",
    "The previous process should generate a scale-free network with a power law degree distribution with\n",
    "exponent $α = 3$.\n",
    "\n",
    "**Plot the degree distribution** of both your generated networks using **cumulative binning** ([see\n",
    "slides 102 and 103](https://www.dcc.fc.up.pt/~pribeiro/aulas/ns2425/2_graphmodels.pdf)) and try to fit with the corresponding power law function (showing it in the plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "$$\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_distribution(a):\n",
    "    \"\"\"Compute the cumulative distribution function (CDF) of a.\"\"\"\n",
    "    sorted_a = np.sort(a)[::-1]\n",
    "    cdf = np.arange(1, len(a) + 1) / len(a)   # [1, n-1/n , n-2/n, ... , 1/n], n = len(degrees)\n",
    "    return sorted_a, cdf\n",
    "\n",
    "# Power-law \n",
    "def power_law(x, a, c):\n",
    "    \"\"\"Power law function for fitting.\"\"\"\n",
    "    return (c/1-a)  * x ** (-(a-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_save_network import load_network_advanced\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import scipy.optimize\n",
    "import os\n",
    "\n",
    "# Load the Barabási-Albert graphs\n",
    "graph1 = load_network_advanced(\"data/ba1.txt\")\n",
    "graph2 = load_network_advanced(\"data/ba2.txt\")\n",
    "\n",
    "# Compute degrees of each node\n",
    "degrees1 = [len(graph1[node]) for node in graph1]\n",
    "np.array(degrees1)\n",
    "degrees2 = [len(graph2[node]) for node in graph2]\n",
    "np.array(degrees2)\n",
    "\n",
    "# Compute cumulative distributions\n",
    "sorted_degrees1, cdf1 = cumulative_distribution(degrees1)\n",
    "sorted_degrees2, cdf2 = cumulative_distribution(degrees2)\n",
    "\n",
    "# Fit power law\n",
    "params1, _ = scipy.optimize.curve_fit(power_law, sorted_degrees1, cdf1)\n",
    "params2, _ = scipy.optimize.curve_fit(power_law, sorted_degrees2, cdf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(sorted_degrees, cdf, params, title):\n",
    "    \"\"\"Plot the cumulative distribution function (CDF) with fitted power law.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_degrees, cdf, 'o', label='CDF')\n",
    "    plt.plot(sorted_degrees, power_law(sorted_degrees, *params), 'r-', label='Fitted Power Law fit: a=%5.3f, c=%5.3f' % tuple(params))\n",
    "    plt.xlabel('Degree (scale)')\n",
    "    plt.ylabel('Cumulative Probability (scale)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cdf_log(sorted_degrees, cdf, params, title):\n",
    "    \"\"\"Plot the cumulative distribution function (CDF) with fitted power law.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(sorted_degrees, cdf, 'o', label='CDF')\n",
    "    plt.plot(sorted_degrees, power_law(sorted_degrees, *params), 'r-', label='Fitted Power Law: a=%5.3f, c=%5.3f' % tuple(params))\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degree (log scale)')\n",
    "    plt.ylabel('Cumulative Probability (log scale)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(sorted_degrees1, cdf1, params1, \"Cumulative Distribution Function for Barabási-Albert Graph 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_log(sorted_degrees1, cdf1, params1, \"Log of Cumulative Distribution Function for Barabási-Albert Graph 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf(sorted_degrees2, cdf2, params2, \"Cumulative Distribution Function for Barabási-Albert Graph 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_log(sorted_degrees2, cdf2, params2, \"Log of Cumulative Distribution Function for Barabási-Albert Graph 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experimenting the powerlaw implementation from 'powerlaw' lib, using the same empirical data we could find a different alfa (~ 3.2). Please check the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "\n",
    "# Fit using powerlaw\n",
    "fit1 = powerlaw.Fit(degrees1, verbose=False)\n",
    "fit2 = powerlaw.Fit(degrees2, verbose=False)\n",
    "\n",
    "# Extract empirical CDFs\n",
    "sorted_degrees1 = np.sort(fit1.data)\n",
    "cdf1 = np.arange(1, len(sorted_degrees1)+1) / len(sorted_degrees1)\n",
    "\n",
    "sorted_degrees2 = np.sort(fit2.data)\n",
    "cdf2 = np.arange(1, len(sorted_degrees2)+1) / len(sorted_degrees2)\n",
    "\n",
    "# Generate theoretical power-law CDFs from xmin onward\n",
    "x1 = np.linspace(fit1.xmin, max(sorted_degrees1), 100)\n",
    "pl_cdf1 = 1 - fit1.power_law.cdf(x1)\n",
    "\n",
    "x2 = np.linspace(fit2.xmin, max(sorted_degrees2), 100)\n",
    "pl_cdf2 = 1 - fit2.power_law.cdf(x2)\n",
    "\n",
    "# Plotting with matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Empirical CDFs\n",
    "plt.plot(sorted_degrees1, 1 - cdf1, 'o', markersize=4, alpha=0.6, label=\"Empirical CDF - Graph 1\")\n",
    "plt.plot(sorted_degrees2, 1 - cdf2, 's', markersize=4, alpha=0.6, label=\"Empirical CDF - Graph 2\")\n",
    "\n",
    "# Fitted power-law CDFs\n",
    "plt.plot(x1, pl_cdf1, 'k--', label=\"Power-law Fit - Graph 1\")\n",
    "plt.plot(x2, pl_cdf2, 'r--', label=\"Power-law Fit - Graph 2\")\n",
    "\n",
    "plt.xlabel(\"Degree (k)\")\n",
    "plt.ylabel(\"P(K ≥ k)\")\n",
    "plt.title(\"Empirical CDF and Power-law Fit (Manual Plot)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', ls='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print power-law parameters\n",
    "print(\"Graph 1: alpha = {:.4f}, xmin = {}\".format(fit1.alpha, fit1.xmin))\n",
    "print(\"Graph 2: alpha = {:.4f}, xmin = {}\".format(fit2.alpha, fit2.xmin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Empirical CDFs\n",
    "plt.loglog(sorted_degrees1, 1 - cdf1, 'o', markersize=4, alpha=0.6, label=\"Empirical CDF - Graph 1\")\n",
    "plt.loglog(sorted_degrees2, 1 - cdf2, 's', markersize=4, alpha=0.6, label=\"Empirical CDF - Graph 2\")\n",
    "\n",
    "# Fitted power-law CDFs\n",
    "plt.loglog(x1, pl_cdf1, 'k--', label=\"Power-law Fit - Graph 1\")\n",
    "plt.loglog(x2, pl_cdf2, 'r--', label=\"Power-law Fit - Graph 2\")\n",
    "\n",
    "plt.xlabel(\"Degree (k)\")\n",
    "plt.ylabel(\"P(K ≥ k)\")\n",
    "plt.title(\"Empirical CDF and Power-law Fit (Manual Plot)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', ls='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print power-law parameters\n",
    "print(\"Graph 1: alpha = {:.4f}, xmin = {}\".format(fit1.alpha, fit1.xmin))\n",
    "print(\"Graph 2: alpha = {:.4f}, xmin = {}\".format(fit2.alpha, fit2.xmin))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
